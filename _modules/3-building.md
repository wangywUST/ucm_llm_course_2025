---
title: Building large language models
---
Mon Jan 31
: [Modeling](../lectures/modeling)
  : **Lecture**{: .label .label-purple } **Discussion**{: .label .label-green }
: *Percy Liang*
: 1. Tokenization
  1. RNNs, Transformers
: Discussion paper:
  - [Transformer-XL: Attentive Language Models beyond a Fixed-Length Context](https://arxiv.org/pdf/1901.02860.pdf)

Wed Feb 2
: [Training](../lectures/training)
  : **Lecture**{: .label .label-purple } **Discussion**{: .label .label-green }
: *Percy Liang*
: 1. Objective functions
  1. Stability
  1. Debugging
: Discussion paper:
  - [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/pdf/2003.10555.pdf)

Mon Feb 7
: [Parallelism](../lectures/parallelism)
  : **Lecture**{: .label .label-purple } **Discussion**{: .label .label-green }
: *Christopher RÃ©*
: 1. Data parallelism
  1. Model parallelism
  1. Pipeline parallelism
: Discussion paper:
  - [DeepSpeed: Extreme-scale model training for everyone](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)

Wed Feb 9
: [Scaling laws](../lectures/scaling-laws)
  : **Lecture**{: .label .label-purple } **Discussion**{: .label .label-green }
: *Tatsunori Hashimoto*
: 1. Scaling laws
: Discussion paper:
  - [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf)

Mon Feb 14
: [Modular architectures](../lectures/selective-architectures)
  : **Lecture**{: .label .label-purple } **Discussion**{: .label .label-green }
: *Percy Liang*
: 1. Mixture of experts
  1. Memory-augmented (retrieval) models
: Discussion paper:
  - [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/pdf/2005.11401.pdf)

Wed Feb 16
: [Adaptation](../lectures/adaptation)
  : **Lecture**{: .label .label-purple } **Discussion**{: .label .label-green }
: *Sang Michael Xie*
: 1. Probing
  1. Fine-tuning
  1. Lightweight fine-tuning
: Discussion paper:
  - [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)

Mon Feb 21
: no class (Presidents' Day)

Wed Feb 23
: [Environmental impact](../lectures-environment)
  : **Lecture**{: .label .label-purple } **Discussion**{: .label .label-green }
: *Percy Liang*
: 1. Training and inference costs
  1. Carbon emissions
: Discussion paper:
  - [Carbon Emissions and Large Neural Network Training](https://arxiv.org/pdf/2104.10350.pdf)
