---
title: Behavior of large language models
---

Mon Jan 3
: [Introduction](../lectures/introduction)
  : **Lecture**{: .label .label-purple }
: *Percy Liang*
: 1. Why does this course exist?
  1. Language models
  1. Overview of the course

Wed Jan 5
: [Capabilities](../lectures/capabilities)
  : **Lecture**{: .label .label-purple } **Discussion**{: .label .label-green }
: *Percy Liang*
: 1. Adaptation framework
  1. Perplexity
  1. Prompting
  1. Overview of GPT-3 tasks
: Discussion paper:
  - [On the Opportunities and Risks of Foundation Models](https://arxiv.org/pdf/2108.07258.pdf) (section 1, excluding 1.4).

Mon Jan 10
: [Harms I](../lectures/harms-1)
  : **Lecture**{: .label .label-purple } **Discussion**{: .label .label-green }
: *Rishi Bommasani*
: 1. Performance disparities
  1. Social bias and stereotypes
: Discussion paper:
  - [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)

Wed Jan 12
: [Harms II](../lectures/harms-2)
  : **Lecture**{: .label .label-purple } **Discussion**{: .label .label-green }
: *Percy Liang*
: 1. Toxicity
  1. Disinformation
: Discussion paper:
  - [RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/pdf/2009.11462.pdf)
