---
layout: page
parent: Lectures
title: Parallelism
nav_order: 4.4
---
This lecture was delivered via whiteboard and slides. A draft of the lecture is provided <a href="../Parallelism.pdf" target="_blank">here</a>. Further supporting discussion on parallelism more generally is given <a href="../An_Ancient_Tale_of_Parallelism.pdf" target="_blank">here</a>.

## Further reading

- [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053.pdf). *M. Shoeybi, M. Patwary, Raul Puri, P. LeGresley, J. Casper, Bryan Catanzaro*. 2019.
- [GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism](https://arxiv.org/pdf/1811.06965.pdf). *Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Z. Chen*. NeurIPS 2018.
- [Efficient large-scale language model training on GPU clusters using Megatron-LM](https://arxiv.org/pdf/2104.04473.pdf). *D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, J. Bernauer, Bryan Catanzaro, Amar Phanishayee, M. Zaharia*. SC 2021.
- [TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models](https://arxiv.org/pdf/2102.07988.pdf). *Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang, D. Song, I. Stoica*. ICML 2021.
